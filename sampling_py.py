# -*- coding: utf-8 -*-
"""Sampling.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KHV_RznwX58MxbnxOp62Gspg_AD5cEMz
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

data=pd.read_csv('Creditcard_data.csv')
data.head()

data['Class'].value_counts()

"""CONVERTING TO BALANCED DATASET

"""

from sklearn.utils import resample

# Separate majority and minority classes
data_majority = data[data.Class == 0]
data_minority = data[data.Class == 1]

# Undersample majority class to match minority
data_majority_downsampled = resample(
    data_majority,
    replace=False,
    n_samples=len(data_minority),
    random_state=42
)

# Combine minority and undersampled majority
data_balanced = pd.concat([data_majority_downsampled, data_minority])

# Shuffle the dataset
data_balanced = data_balanced.sample(frac=1, random_state=42)

# Check balance
data_balanced['Class'].value_counts()

"""Creating Samples

"""

samples = []

for i in range(5):
    sample = data_balanced.sample(frac=0.8, random_state=10+i)
    samples.append(sample)

from sklearn.model_selection import train_test_split
import numpy as np

def sampling1(data):
    return data.sample(frac=1, random_state=1)

def sampling2(data):
    train, _ = train_test_split(
        data,
        test_size=0.2,
        stratify=data['Class'],
        random_state=2
    )
    return train

def sampling3(data):
    return data.sample(n=len(data), replace=True, random_state=3)

def sampling4(data):
    return data.iloc[::2]

def sampling5(data, n_clusters=10, clusters_to_select=5):
    data = data.copy()
    data['cluster_id'] = data.index % n_clusters

    selected_clusters = np.random.choice(
        data['cluster_id'].unique(),
        size=clusters_to_select,
        replace=False
    )

    return data[data['cluster_id'].isin(selected_clusters)].drop(columns=['cluster_id'])

sampling1(data)
sampling2(data)
sampling3(data)
sampling4(data)
sampling5(data)

"""Defining five models"""

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC

models = {
    "M1": LogisticRegression(max_iter=1000),
    "M2": DecisionTreeClassifier(),
    "M3": RandomForestClassifier(),
    "M4": KNeighborsClassifier(n_neighbors=3),
    "M5": SVC()
}

from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

sampling_methods = {
    "Sampling1": sampling1,
    "Sampling2": sampling2,
    "Sampling3": sampling3,
    "Sampling4": sampling4,
    "Sampling5": sampling5
}

results = {}

for model_name, model in models.items():
    results[model_name] = {}

    for samp_name, samp_func in sampling_methods.items():
        # apply sampling on balanced data
        data_sampled = samp_func(data_balanced)

        # split features and target
        X = data_sampled.drop('Class', axis=1)
        y = data_sampled['Class']

        # train-test split
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42
        )

        # train model
        model.fit(X_train, y_train)

        # predict
        y_pred = model.predict(X_test)

        # accuracy
        acc = accuracy_score(y_test, y_pred) * 100
        results[model_name][samp_name] = round(acc, 2)

results_data = pd.DataFrame(results).T
results_data

results_data.idxmax(axis=1)

"""OVERALL MAX ACCURACY"""

results_data.max().max()

results_data.stack().idxmax()

plt.figure(figsize=(10, 6))

# Plot one line per model
for model in results_data.index:
    plt.plot(
        results_data.columns,
        results_data.loc[model],
        marker='o',
        label=model
    )

# Labels and title
plt.xlabel("Sampling Techniques")
plt.ylabel("Accuracy (%)")
plt.title("Accuracy Comparison of Sampling Techniques vs ML Models")
plt.legend()
plt.grid(True)

# Save the figure
plt.savefig("sampling_vs_models_accuracy.png", dpi=300, bbox_inches="tight")

# Show the plot
plt.show()

results_data.plot(kind='bar', figsize=(10,6))
plt.xlabel("Models")
plt.ylabel("Accuracy (%)")
plt.title("Accuracy Comparison of Sampling Techniques")
plt.legend(title="Sampling Techniques")
plt.grid(axis='y')
plt.savefig("sampling_bar_plot.png", dpi=300, bbox_inches="tight")
plt.show()

best_sampling = results_data.idxmax(axis=1)
best_accuracy = results_data.max(axis=1)

plt.figure(figsize=(8,5))
plt.bar(best_sampling.index, best_accuracy)
plt.xlabel("Models")
plt.ylabel("Best Accuracy (%)")
plt.title("Best Sampling Technique for Each Model")
plt.grid(axis='y')
plt.savefig("best_sampling_per_model.png", dpi=300, bbox_inches="tight")
plt.show()

avg_accuracy = results_data.mean(axis=0)
ranking = avg_accuracy.sort_values(ascending=False)

plt.figure(figsize=(8,5))
plt.bar(ranking.index, ranking.values)
plt.xlabel("Sampling Techniques")
plt.ylabel("Average Accuracy (%)")
plt.title("Overall Ranking of Sampling Techniques")
plt.grid(axis='y')
plt.savefig("sampling_ranking.png", dpi=300, bbox_inches="tight")
plt.show()

avg_accuracy = results_data.mean(axis=0)

plt.figure(figsize=(8, 5))
plt.bar(avg_accuracy.index, avg_accuracy.values)

plt.xlabel("Sampling Techniques")
plt.ylabel("Average Accuracy (%)")
plt.title("Average Accuracy per Sampling Technique")
plt.grid(axis='y')

plt.savefig("average_accuracy_sampling.png", dpi=300, bbox_inches="tight")
plt.show()

variance = results_data.var(axis=0)

plt.figure(figsize=(8, 5))
plt.bar(variance.index, variance.values)

plt.xlabel("Sampling Techniques")
plt.ylabel("Accuracy Variance")
plt.title("Sampling Technique Stability Plot (Variance)")
plt.grid(axis='y')

plt.savefig("sampling_variance.png", dpi=300, bbox_inches="tight")
plt.show()